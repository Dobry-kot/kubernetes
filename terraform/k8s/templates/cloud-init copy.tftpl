#cloud-config
users:
  - name: dkot
    sudo: ALL=(ALL) NOPASSWD:ALL
    groups: users, admin
    shell: /bin/bash
    lock_passwd: true
    ssh_authorized_keys:
      - ${ ssh_key }

runcmd:
 - [ /usr/bin/key-keeper, -config, /etc/kubernetes/pki/vault-config ]

write_files:
  - path: /etc/kubernetes/manifests/etcd.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://${ instance_name }.${base_domain}:2379
        creationTimestamp: null
        labels:
          component: etcd
          tier: control-plane
        name: etcd
        namespace: kube-system
      spec:
        containers:
        - command:
            - etcd
          args:
            - --trusted-ca-file=/etc/kubernetes/pki/ca/etcd-ca.pem
            - --cert-file=/etc/kubernetes/pki/certs/etcd/system:etcd-server.pem
            - --key-file=/etc/kubernetes/pki/certs/etcd/system:etcd-server-key.pem
            - --peer-trusted-ca-file=/etc/kubernetes/pki/ca/etcd-ca.pem
            - --peer-cert-file=/etc/kubernetes/pki/certs/etcd/system:etcd-peer.pem
            - --peer-key-file=/etc/kubernetes/pki/certs/etcd/system:etcd-peer-key.pem
            - --name=${ instance_name }.${base_domain}
            - --client-cert-auth=true
            - --initial-advertise-peer-urls=https://${ instance_name }.${base_domain}:2380
            - --listen-client-urls=https://0.0.0.0:2379
            - --listen-peer-urls=https://0.0.0.0:2380
            - --listen-metrics-urls=http://0.0.0.0:2381
            - --advertise-client-urls=https://${ instance_name }.${base_domain}:2379
            - --initial-cluster-token=etcd
            - --initial-cluster=${ etcd_initial_cluster }
            - --initial-cluster-state=new
            - --data-dir=/var/lib/etcd
            - --strict-reconfig-check
            - --peer-client-cert-auth=true
            - --peer-auto-tls=true
          image: k8s.gcr.io/etcd:3.5.3-0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              host: 127.0.0.1
              path: /health
              port: 2381
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          name: etcd
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          startupProbe:
            failureThreshold: 24
            httpGet:
              host: 127.0.0.1
              path: /health
              port: 2381
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /var/lib/etcd
            name: etcd-data
          - mountPath: /etc/kubernetes/pki/certs/etcd
            name: etcd-certs
          - mountPath: /etc/kubernetes/pki/ca
            name: ca
        hostNetwork: true
        priorityClassName: system-node-critical
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        volumes:
        - hostPath:
            path: /etc/kubernetes/pki/certs/etcd
            type: DirectoryOrCreate
          name: etcd-certs
        - hostPath:
            path: /etc/kubernetes/pki/ca
            type: DirectoryOrCreate
          name: ca
        - hostPath:
            path: /var/lib/etcd
            type: DirectoryOrCreate
          name: etcd-data
      status: {}

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: v1
      kind: Pod
      metadata:
        # %{~ for api_spec in lb_api_ip ~}
        # %{~ if api_spec.port == 6443 ~}
        # %{~ for api_ip in api_spec.external_address_spec ~}
        # annotations:
        #   kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: "${api_ip.address}"
        # %{~ endfor ~}  
        # %{~ endif ~}
        # %{~ endfor ~}  
        creationTimestamp: null
        labels:
          component: kube-apiserver
          tier: control-plane
        name: kube-apiserver
        namespace: kube-system
      spec:
        containers:
        - command:
          - kube-apiserver     
          args:
            - --etcd-servers=${etcd_advertise_client_urls}
            - --cloud-provider=external
            - --tls-cert-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-server.pem
            - --tls-private-key-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-server-key.pem
            - --client-ca-file=/etc/kubernetes/pki/ca/root-ca.pem
            - --etcd-cafile=/etc/kubernetes/pki/ca/etcd-ca.pem
            - --etcd-certfile=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-etcd-client.pem
            - --etcd-keyfile=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-etcd-client-key.pem
            - --kubelet-client-certificate=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-kubelet-client.pem
            - --kubelet-client-key=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-kubelet-client-key.pem
            - --proxy-client-cert-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-front-proxy-client.pem
            - --proxy-client-key-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-front-proxy-client-key.pem
            - --requestheader-client-ca-file=/etc/kubernetes/pki/ca/front-proxy-ca.pem
            - --service-account-key-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-sa.pem
            - --service-account-signing-key-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-sa-key.pem
            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
            - --kubelet-timeout=5s
            - --v=2
            - --service-cluster-ip-range=29.64.0.0/16
            - --secure-port=6443
            # %{~ for api_spec in lb_api_ip ~}
            # %{~ if api_spec.port == 6443 ~}
            # %{~ for api_ip in api_spec.external_address_spec ~}
            # - --advertise-address=${api_ip.address}
            # %{~ endfor ~}  
            # %{~ endif ~}
            # %{~ endfor ~}
            - --advertise-address=29.64.0.1
            - --requestheader-allowed-names=front-proxy-client
            - --requestheader-extra-headers-prefix=X-Remote-Extra-
            - --requestheader-group-headers=X-Remote-Group
            - --requestheader-username-headers=X-Remote-User
            - --requestheader-allowed-names=aggregator
            - --allow-privileged=true
            - --authorization-mode=Node,RBAC
            - --bind-address=0.0.0.0
            - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,AlwaysPullImages,NodeRestriction
            - --enable-bootstrap-token-auth=true
            - --runtime-config=api/all=true
            - --enable-aggregator-routing=true 
            - --api-audiences=system:konnectivity-server
            - --service-account-issuer=https://kubernetes.default.svc.cluster.local
            - --anonymous-auth=true
            - --audit-log-maxage=30
            - --audit-log-maxbackup=10
            - --audit-log-maxsize=1000
            - --audit-log-mode=batch
            - --audit-policy-file=/etc/kubernetes/kube-apiserver/audit-policy.yaml
            - --event-ttl=1h0m0s
            - --kubernetes-service-node-port=0
            - --master-service-namespace=default
            - --max-connection-bytes-per-sec=0
            - --max-requests-inflight=400
            - --min-request-timeout=1800
            - --profiling=false
            - --feature-gates=RotateKubeletServerCertificate=true
          image: k8s.gcr.io/kube-apiserver:v1.23.5
                 
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              host: 127.0.0.1
              path: /livez
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          name: kube-apiserver
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /readyz
              port: 6443
              scheme: HTTPS
            periodSeconds: 1
            timeoutSeconds: 15
          resources:
            requests:
              cpu: 250m
          startupProbe:
            failureThreshold: 24
            httpGet:
              host: 127.0.0.1
              path: /livez
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ca-certs
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-ca-certificates
            readOnly: true
          - mountPath: /etc/kubernetes/pki/certs
            name: k8s-certs
            readOnly: true
          - mountPath: /etc/kubernetes/pki/ca
            name: k8s-ca
            readOnly: true
          - mountPath: /etc/kubernetes/kube-apiserver
            name: k8s-kube-apiserver-configs
            readOnly: true
          - mountPath: /usr/local/share/ca-certificates
            name: usr-local-share-ca-certificates
            readOnly: true
          - mountPath: /usr/share/ca-certificates
            name: usr-share-ca-certificates
            readOnly: true
        hostNetwork: true
        priorityClassName: system-node-critical
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: DirectoryOrCreate
          name: ca-certs
        - hostPath:
            path: /etc/ca-certificates
            type: DirectoryOrCreate
          name: etc-ca-certificates
        - hostPath:
            path: /etc/kubernetes/pki/certs
            type: DirectoryOrCreate
          name: k8s-certs
        - hostPath:
            path: /etc/kubernetes/pki/ca
            type: DirectoryOrCreate
          name: k8s-ca
        - hostPath:
            path: /etc/kubernetes/kube-apiserver
            type: DirectoryOrCreate
          name: k8s-kube-apiserver-configs
        - hostPath:
            path: /usr/local/share/ca-certificates
            type: DirectoryOrCreate
          name: usr-local-share-ca-certificates
        - hostPath:
            path: /usr/share/ca-certificates
            type: DirectoryOrCreate
          name: usr-share-ca-certificates
      status: {}

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: v1
      kind: Pod
      metadata:
        creationTimestamp: null
        labels:
          component: kube-controller-manager
          tier: control-plane
        name: kube-controller-manager
        namespace: kube-system
      spec:
        containers:
        - command:
            - kube-controller-manager
          args:
            - --tls-cert-file=/etc/kubernetes/pki/certs/kube-controller-manager/system:kube-controller-manager-server.pem 
            - --tls-private-key-file=/etc/kubernetes/pki/certs/kube-controller-manager/system:kube-controller-manager-server-key.pem 
            - --client-ca-file=/etc/kubernetes/pki/ca/root-ca.pem 
            - --cluster-signing-cert-file=/etc/kubernetes/pki/ca/root-ca.pem 
            - --cluster-signing-key-file=/etc/kubernetes/pki/ca/root-ca-key.pem 
            - --requestheader-client-ca-file=/etc/kubernetes/pki/ca/front-proxy-ca.pem 
            - --service-account-private-key-file=/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-kubelet-client-key.pem 
            - --kubeconfig=/etc/kubernetes/kube-controller-manager/kubeconfig 
            - --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager/kubeconfig 
            - --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager/kubeconfig
            - --root-ca-file=/etc/kubernetes/pki/ca/root-ca.pem
            - --bind-address=0.0.0.0
            - --secure-port=10257
            - --allocate-node-cidrs=true
            - --cluster-cidr=29.64.0.0/16
            - --cluster-name=kubernetes
            - --concurrent-deployment-syncs=5
            - --concurrent-endpoint-syncs=5
            - --concurrent-namespace-syncs=10
            - --concurrent-replicaset-syncs=20
            - --concurrent-resource-quota-syncs=5
            - --horizontal-pod-autoscaler-sync-period=30s
            - --kube-api-burst=120
            - --kube-api-qps=100
            - --leader-elect=true
            - --leader-elect-lease-duration=15s
            - --leader-elect-renew-deadline=10s
            - --leader-elect-retry-period=2s
            - --namespace-sync-period=2m0s
            - --node-cidr-mask-size=24
            - --node-monitor-grace-period=40s
            - --node-monitor-period=5s
            - --node-startup-grace-period=1m0s
            - --pod-eviction-timeout=30s
            - --profiling=false
            - --resource-quota-sync-period=5m0s
            - --terminated-pod-gc-threshold=0
            - --use-service-account-credentials=true
            - --controllers=bootstrapsigner,tokencleaner,cloud-node-lifecycle,csrapproving,csrcleaner,csrsigning
            - --authorization-always-allow-paths=/healthz,/metrics
            - --feature-gates=RotateKubeletServerCertificate=true
            - --v=2
          image: k8s.gcr.io/kube-controller-manager:v1.23.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10257
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          name: kube-controller-manager
          resources:
            requests:
              cpu: 200m
          startupProbe:
            failureThreshold: 24
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10257
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ca-certs
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-ca-certificates
            readOnly: true
          - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
            name: flexvolume-dir
          - mountPath: /etc/kubernetes/pki/certs
            name: k8s-certs
            readOnly: true
          - mountPath: /etc/kubernetes/pki/ca
            name: k8s-ca
            readOnly: true
          - mountPath: /etc/kubernetes/kube-controller-manager
            name: k8s-kube-controller-manager-configs
            readOnly: true
          - mountPath: /usr/local/share/ca-certificates
            name: usr-local-share-ca-certificates
            readOnly: true
          - mountPath: /usr/share/ca-certificates
            name: usr-share-ca-certificates
            readOnly: true
        hostNetwork: true
        priorityClassName: system-node-critical
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: DirectoryOrCreate
          name: ca-certs
        - hostPath:
            path: /etc/ca-certificates
            type: DirectoryOrCreate
          name: etc-ca-certificates
        - hostPath:
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
            type: DirectoryOrCreate
          name: flexvolume-dir
        - hostPath:
            path: /etc/kubernetes/pki/certs
            type: DirectoryOrCreate
          name: k8s-certs
        - hostPath:
            path: /etc/kubernetes/pki/ca
            type: DirectoryOrCreate
          name: k8s-ca
        - hostPath:
            path: /etc/kubernetes/kube-controller-manager
            type: DirectoryOrCreate
          name: k8s-kube-controller-manager-configs
        - hostPath:
            path: /usr/local/share/ca-certificates
            type: DirectoryOrCreate
          name: usr-local-share-ca-certificates
        - hostPath:
            path: /usr/share/ca-certificates
            type: DirectoryOrCreate
          name: usr-share-ca-certificates
      status: {}

  - path: /etc/kubernetes/kube-apiserver/audit-policy.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
      - level: Metadata
      - level: RequestResponse


  - path: /etc/kubernetes/kube-apiserver/egress-selector-configuration.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: apiserver.k8s.io/v1beta1
      kind: EgressSelectorConfiguration
      egressSelections:
      - name: cluster
          connection:
          proxyProtocol: HTTPConnect
          transport:
              tcp:
                  url: "https://konnectivity-server-svc:8131"
                  TLSConfig:
                      caBundle: /pki/konnectivity-client/ca.crt
                      clientKey: /pki/konnectivity-client/tls.key
                      clientCert: /pki/konnectivity-client/tls.crt
      - name: master
          connection:
          proxyProtocol: Direct
      - name: etcd
          connection:
          proxyProtocol: Direct


  - path: /etc/kubernetes/kube-apiserver/kubeconfig
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority: /etc/kubernetes/pki/ca/root-ca.pem
          server: https://127.0.0.1:6443
        name: kubernetes
      contexts:
      - context:
          cluster: kubernetes
          namespace: default
          user: system:kube-apiserver
        name: system:kube-apiserver@kubernetes
      current-context: system:kube-apiserver@kubernetes
      kind: Config
      preferences: {}
      users:
      - name: system:kube-apiserver
        user:
          client-certificate: /etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-kubelet-client.pem
          client-key: /etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-kubelet-client-key.pem


  - path: /etc/kubernetes/pki/vault-config
    owner: root:root
    permissions: '0644'
    content: |
      ---
      vault:
        address: "http://51.250.67.8:9200"
        bootstrap_token: ${temporary_token}
        local_path_to_role_id: "/etc/kubernetes/pki/role_id"
        local_path_to_secret_id: "/etc/kubernetes/pki/secret_id"
        approle_path: clusters/cluster-1/approle
        approle_name: test-role
        request_timeout: "10m"
      certificates:
        vault_kv: "clusters/cluster-1/kv"
        reissue_interval: "24h"
        root_ca:
          - common_name: "Root Cluster-1"
            root_path_ca: "clusters/cluster-1/pki/root"
        intermediate_ca:
          - common_name: "kubernetes"
            root_path_ca: "clusters/cluster-1/pki/root"
            cert_path:  "clusters/cluster-1/pki/kubernetes"
            host_path: "/etc/kubernetes/pki/ca/root-ca"
            exported_key: false
            generate: true
          - common_name: "etcd"
            root_path_ca: "clusters/cluster-1/pki/root"
            cert_path:  "clusters/cluster-1/pki/etcd"
            host_path: "/etc/kubernetes/pki/ca/etcd-ca"
            exported_key: false
            generate: true
          - common_name: "front-proxy"
            root_path_ca: "clusters/cluster-1/pki/root"
            cert_path:  "clusters/cluster-1/pki/kube-apiserver"
            host_path: "/etc/kubernetes/pki/ca/front-proxy-ca"
            exported_key: false
            generate: true
        csr:
        # KUBE-APISERVER -->
          - common_name: "system:kube-apiserver-front-proxy-client"
            role: "base-role" # system_masters_client
            host_path: "/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-front-proxy-client"
            cert_path: "clusters/cluster-1/pki/kube-apiserver"

          - common_name: "system:kube-apiserver-sa"
            role: "base-role" # system_kube_apiserver_sa
            host_path: "/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-sa"
            cert_path: "clusters/cluster-1/pki/kubernetes"

          - common_name: "system:kube-apiserver-etcd-client"
            role: "base-role" # system_masters_client
            host_path: "/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-etcd-client"
            cert_path: "clusters/cluster-1/pki/etcd"

          - common_name: "system:kube-apiserver-server"
            role: "base-role" # system_kube_apiserver_server
            host_path: "/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-server"
            cert_path: "clusters/cluster-1/pki/kubernetes"
            hosts:
              - "localhost"
              - "kubernetes"
              - "kubernetes.default"
              - "kubernetes.default.svc"
              - "kubernetes.default.svc.cluster"
              - "kubernetes.default.svc.cluster.local"
            ips:
              - "127.0.0.1"
              - "127.0.1.1"
              - "127.0.1.6"

          - common_name: "system:kube-apiserver-kubelet-client"
            role: "base-role" # system_masters_client
            host_path: "/etc/kubernetes/pki/certs/kube-apiserver/system:kube-apiserver-kubelet-client"
            cert_path: "clusters/cluster-1/pki/kubernetes"

        # KUBE-CONTROLLER-MANAGER -->
          - common_name: "system:kube-controller-manager"
            role: "base-role" # system_kube_controller_manager_client
            host_path: "/etc/kubernetes/pki/certs/kube-controller-manager/system:kube-controller-manager"
            cert_path: "clusters/cluster-1/pki/kubernetes"

          - common_name: "system:kube-controller-manager-server"
            role: "base-role" # system_kube_controller_manager_server
            host_path: "/etc/kubernetes/pki/certs/kube-controller-manager/system:kube-controller-manager-server"
            cert_path: "clusters/cluster-1/pki/kubernetes"
            hosts:
              - "localhost"
              - "kube-controller-manager.default"
              - "kube-controller-manager.default.svc"
              - "kube-controller-manager.default.svc.cluster"
              - "kube-controller-manager.default.svc.cluster.local"
            ips:
              - "127.0.0.1"
              - "127.0.1.1"
              - "127.0.1.6"

        # KUBE-SHEDULER -->
          - common_name: "system:kube-scheduler"
            role: "base-role" # system_kube_scheduler_client
            host_path: "/etc/kubernetes/pki/certs/kube-scheduler/system:kube-scheduler-client"
            cert_path: "clusters/cluster-1/pki/kubernetes"

          - common_name: "system:kube-scheduler-server"
            role: "base-role" # system_kube_scheduler_server
            host_path: "/etc/kubernetes/pki/certs/kube-scheduler/system:kube-scheduler-server"
            cert_path: "clusters/cluster-1/pki/kubernetes"
            hosts:
              - "localhost"
              - "kube-scheduler.default"
              - "kube-scheduler.default.svc"
              - "kube-scheduler.default.svc.cluster"
              - "kube-scheduler.default.svc.cluster.local"
            ips:
              - "127.0.0.1"
              - "127.0.1.1"
              - "127.0.1.6"

        # KUBELET -->
          - common_name: "system:node:${ instance_name }.${base_domain}"
            role: "base-role" # system_node_client
            host_path: "/etc/kubernetes/pki/certs/kubelet/current:kubelet"
            cert_path: "clusters/cluster-1/pki/kubernetes"

        # ETCD -->
          - common_name: "system:etcd-healthcheck-client"
            role: "base-role" # system_etcd_client
            host_path: "/etc/kubernetes/pki/certs/etcd/system:etcd-healthcheck-client"
            cert_path: "clusters/cluster-1/pki/etcd"

          - common_name: "system:etcd-peer"
            role: "base-role" # system_etcd_peer
            host_path: "/etc/kubernetes/pki/certs/etcd/system:etcd-peer"
            cert_path: "clusters/cluster-1/pki/etcd"
            hosts:
              - "localhost"
          %{~ for master in list_masters  ~}
              - ${indent(2, master)}
          %{~ endfor ~}
            ips:
              - "127.0.0.1"
              - "127.0.1.1"
              - "127.0.1.6"

          - common_name: "system:etcd-server"
            role: "base-role" # system_etcd_server
            host_path: "/etc/kubernetes/pki/certs/etcd/system:etcd-server"
            cert_path: "clusters/cluster-1/pki/etcd"
            hosts:
              - "localhost"
          %{~ for master in list_masters  ~}
              - ${indent(2, master)}
          %{~ endfor ~}
            ips:
              - "127.0.0.1"
              - "127.0.1.1"
              - "127.0.1.6"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: create-csrs-for-bootstrapping
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:node-bootstrapper
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: "MjAyMi0xMC0xMFQwMzoyMjoxMVo="
  token-id: "ZmlyZWN1YmUtbWFzdGVyLTE="
  token-secret: "MmU4ZGUzNDY3ZDA3MGYyMA=="
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==
kind: Secret
metadata:
  name: bootstrap-token-master-0
  namespace: kube-system
type: bootstrap.kubernetes.io/token
---
apiVersion: v1
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: "MjAyMi0xMC0xMFQwMzoyMjoxMVo="
  token-id: "ZmlyZWN1YmUtbWFzdGVyLTI="
  token-secret: "MmU4ZGUzNDY3ZDA3MGYyMA=="
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==
kind: Secret
metadata:
  name: bootstrap-token-master-1
  namespace: kube-system
type: bootstrap.kubernetes.io/token
---
apiVersion: v1
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: "MjAyMi0xMC0xMFQwMzoyMjoxMVo="
  token-id: "ZmlyZWN1YmUtbWFzdGVyLTM="
  token-secret: "MmU4ZGUzNDY3ZDA3MGYyMA=="
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==
kind: Secret
metadata:
  name: bootstrap-token-master-2
  namespace: kube-system
type: bootstrap.kubernetes.io/token
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: auto-approve-csrs-for-group
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: auto-approve-renewals-for-nodes
subjects:
- kind: Group
  name: system:nodes
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
  apiGroup: rbac.authorization.k8s.io